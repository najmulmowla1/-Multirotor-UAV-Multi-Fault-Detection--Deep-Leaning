# -*- coding: utf-8 -*-
"""EMHSAM-CLN_modelipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zVae5EOaeK-icIQoIhJPKTONBG5Ttyd3
"""

# ---------- Build EMHSAM-CLN model (stacked CNN + LSTM + EMHSA) ----------
def build_emhsam_cln(n_features: int, num_classes: int):

    input_shape = (n_features, 1)

    # CNN branch (stacked conv layers; padding='same' avoids length collapse)
    cnn_in = Input(shape=input_shape, name="cnn_in")
    x = cnn_in
    for f, k in [(64, 3), (64, 3), (128, 5), (256, 5), (64, 5), (32, 5)]:
        x = Conv1D(filters=f, kernel_size=k, activation='relu', padding='same')(x)
    cnn_feat = GlobalAveragePooling1D()(x)
    cnn_feat = Dense(64, activation='relu')(cnn_feat)
    cnn_feat = Dense(32, activation='relu')(cnn_feat)

    # LSTM + EMHSA branch
    lstm_in = Input(shape=input_shape, name="lstm_in")
    y = LSTM(units=32, return_sequences=True)(lstm_in)
    y = EnhancedMultiHeadSelfAttention(units=32, num_heads=4)(y)
    lstm_feat = GlobalAveragePooling1D()(y)

    # Fusion + classifier
    merged = Concatenate()([cnn_feat, lstm_feat])
    merged = Dense(64, activation='relu')(merged)
    out = Dense(num_classes, activation='softmax')(merged)

    model = Model(inputs=[cnn_in, lstm_in], outputs=out)

    # Optimizer initial LR
    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model